{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgojiqdMXZ8H"
   },
   "source": [
    "# Problem 1 (10 pts):\n",
    "\n",
    "From Maximum Likelihood to Cross-Entropy Loss\n",
    "Learning Objectives: Connect probability theory to loss functions, understand why cross-entropy emerges naturally.\n",
    "\n",
    "Part A: Binary Classification Loss Derivation\n",
    "\n",
    "Setup: We have $n$ data points $\\{ (x_i, y_i) \\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{ 0, 1 \\}$. Assume your model outputs the probability of class 1 as $p_i = p(y_i = 1 \\mid x_i) = \\sigma(w^T x_i + b)$ where $w \\in \\mathbb{R}^d$, $b \\in \\mathbb{R}$, and $\\sigma(z)$ is the sigmoid function $\\sigma(z) = 1 / (1 + e^{-z})$.\n",
    "\n",
    "1. Derive from MLE:\n",
    "    * Write the likelihood function for the dataset\n",
    "    * Take the log-likelihood\n",
    "    * Show that maximizing log-likelihood = minimizing binary cross-entropy\n",
    "    * Bonus (5 pts): Derive the gradient and show it has the nice form: $\\nabla_w BCE = X^T(p - y)$\n",
    "\n",
    "Part B: Extension to Multi-class\n",
    "\n",
    "1. Softmax derivation: Extend to $K$ classes using softmax function\n",
    "    * Likelihood\n",
    "    * Log-likelihood\n",
    "    * Maximizing log-likelihood = minimizing categorical cross-entropy\n",
    "\n",
    "Part C: Implement dependencies in `hw1_impl.py` for function `problem_1_part_c` in `hw1_script.py`. You will implement both binary and multi-class cross-entropy from scratch. You will compare your implementation with `sklearn.metrics.log_loss`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvdT93egXiKT"
   },
   "source": [
    "# Problem 2 (10 pts): Normal Equations vs. Gradient Descent - A Computational Study\n",
    "Learning Objectives: Understand trade-offs between analytical and iterative solutions.\n",
    "\n",
    "Implement dependencies in `hw1_impl.py` for function `problem_2` in `hw1_script.py`.\n",
    "\n",
    "Analysis Tasks:\n",
    "\n",
    "1. Answer this question. Memory Usage: When does the normal equation become impractical?\n",
    "2. Conditioning: What happens when $X^TX$ is nearly singular? Add ridge regularization.\n",
    "3. Report: When would you choose each method in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6eGQF_5XeNY"
   },
   "source": [
    "# Problem 3 (10 pts): SGD Exploration - Escaping Local Minima (Extended)\n",
    "Learning Objectives: Understand SGD's stochastic nature and hyperparameter effects.\n",
    "\n",
    "Due to history, there is no Part A.\n",
    "\n",
    "Part B: Implement dependencies in `hw1_impl.py` for functions `problem_3_part_b` and `problem_3_part_c` in `hw1_script.py`.\n",
    "\n",
    "Part D: Analysis Questions\n",
    "\n",
    "1. What batch size gives the best exploration vs. exploitation trade-off?\n",
    "2. How does the \"escape probability\" change with learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9_VboSURl6f"
   },
   "source": [
    "# Problem 4 (10 pts): The Perceptron Problem - Understanding Linear Separability Limitations\n",
    "\n",
    "## What is a Perceptron?\n",
    "\n",
    "Based on our lecture, a **perceptron** is a binary classifier that makes predictions using a linear decision boundary. It consists of:\n",
    "\n",
    "- **Inputs**: A feature vector $x \\in \\mathbb{R}^d$\n",
    "- **Weights**: A weight vector $w \\in \\mathbb{R}^d$\n",
    "- **Bias**: A scalar bias term $b \\in \\mathbb{R}$\n",
    "- **Activation**: A step function (threshold function)\n",
    "\n",
    "The perceptron computes:\n",
    "$$ f(x) = \\text{step} (w^T x + b) $$\n",
    "\n",
    "Where the step function outputs:\n",
    "$$ \\text{step}(w^T x + b) = \\begin{cases}\n",
    "1 & \\text{if} \\quad w^T x + b \\geq 0 \\\\\n",
    "0 & \\text{if} \\quad w^T x + b < 0\n",
    "\\end{cases} $$\n",
    "\n",
    "The decision boundary is the hyperplane defined by $w^T x + b = 0$, which divides the input space into two regions.\n",
    "\n",
    "## The Fundamental Problem\n",
    "\n",
    "The perceptron suffers from a **critical limitation**: it can only solve **linearly separable** problems. This means it can only correctly classify data where the two classes can be perfectly separated by a single straight line (in 2D) or hyperplane (in higher dimensions).\n",
    "\n",
    "### The XOR Problem: A Classic Example\n",
    "\n",
    "The most famous demonstration of this limitation is the **XOR (Exclusive OR) problem**:\n",
    "\n",
    "| x₁ | x₂ | XOR Output |\n",
    "|----|----|------------|\n",
    "| 0  | 0  | 0          |\n",
    "| 0  | 1  | 1          |\n",
    "| 1  | 0  | 1          |\n",
    "| 1  | 1  | 0          |\n",
    "\n",
    "If you plot these four points:\n",
    "- Points (0,1) and (1,0) should be classified as class 1 (XOR = 1)\n",
    "- Points (0,0) and (1,1) should be classified as class 0 (XOR = 0)\n",
    "\n",
    "**No single straight line can separate these classes!** The pattern requires a non-linear decision boundary.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "This limitation reveals why:\n",
    "\n",
    "1. **Single perceptrons are insufficient** for many real-world problems\n",
    "2. **We need non-linearity** in our models (like ReLU activation functions)\n",
    "3. **Multiple layers are essential** to create complex, non-linear decision boundaries\n",
    "4. **The XOR problem motivated** the development of multi-layer neural networks\n",
    "\n",
    "As we learned in our previous lecture, when we combine multiple ReLU neurons and stack them in layers, we can create complex, bent decision boundaries that can solve non-linearly separable problems like XOR.\n",
    "\n",
    "This historical limitation of the perceptron was so significant that it contributed to the \"AI winter\" of the 1970s, until researchers developed multi-layer networks with backpropagation in the 1980s.\n",
    "\n",
    "Learning Objectives\n",
    "\n",
    "1. Implement a perceptron from scratch to understand its mechanics\n",
    "2. Demonstrate why linear models fail on non-linearly separable data\n",
    "3. Visualize decision boundaries and their limitations\n",
    "4. Show how adding non-linear features can solve the problem\n",
    "\n",
    "Implement dependencies in `hw1_impl.py` for functions `problem_4` in `hw1_script.py`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
